{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "IEEE_MLC_DATASET_LOAD_EXAMPLE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyaMGniRAktB"
      },
      "source": [
        "#Import TensorBoard For Logging\r\n",
        "%load_ext tensorboard "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc8XoMvfiduN"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import datetime\n",
        "import gc\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "# import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enro9AUmi63j"
      },
      "source": [
        "# Data Reading / Pre-Processing (Unlabeled Data)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1skzF3ariq3n"
      },
      "source": [
        "## DATA Import Code (Via Drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BklvtBZai3mL",
        "outputId": "0b0159f4-197e-427e-9a1b-40330676a868"
      },
      "source": [
        "#Mount Drive\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUlMRPCUq9cI"
      },
      "source": [
        "## Function to Read Unlabelled Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfVDrYCwidu8"
      },
      "source": [
        "CTW_unlabelled = \"/content/drive/My Drive/Datasets/CTW2020/Unzipped/CTW2020_unlabelled_data/\"\r\n",
        "# path might vary based on how you saved data in your drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU_0wrnMidu_"
      },
      "source": [
        "def get_data(data_file):\n",
        "    \n",
        "    f = h5py.File(data_file, 'r')\n",
        "    H_Re = f['H_Re'][:] #shape (sample size, 56, 924, 5)\n",
        "    H_Im = f['H_Im'][:] #shape (sample size, 56, 924, 5)\n",
        "    SNR = f['SNR'][:] #shape (sample size, 56, 5)\n",
        "    f.close()\n",
        "            \n",
        "    return H_Re, H_Im, SNR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK0P2T8cui2U",
        "outputId": "53c3c246-5c13-41c7-bfbd-ceaa34a30963"
      },
      "source": [
        "#lists all the files\r\n",
        "\r\n",
        "for dirname, _, filenames in os.walk(CTW_unlabelled):\r\n",
        "    # for filename in filenames:\r\n",
        "    #     print(os.path.join(dirname, filename))\r\n",
        "    print(\"Total number of files = {}\".format(len(filenames)))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of files = 64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev2YGZDcngTV"
      },
      "source": [
        "## Calling with Start as 1,2,3 will load in distinct sets of 10 (Or mentioned) files \r\n",
        "\r\n",
        "def LoadData1():\r\n",
        "  H_Re=[]\r\n",
        "  H_Im=[]\r\n",
        "  SNR=[]\r\n",
        "  #print(\"Loading Files\")\r\n",
        "  while True:\r\n",
        "    for i in range(64):\r\n",
        "      filename = CTW_unlabelled + \"file_{}.hdf5\".format(i+1)\r\n",
        "      H_Re, H_Im, SNR= get_data(filename)\r\n",
        "      i+=1\r\n",
        "      # YOUR_CODE\r\n",
        "      amplitude = \r\n",
        "      filtered_amp = \r\n",
        "      normalized_amp =\r\n",
        "\r\n",
        "      filtered_snr = \r\n",
        "      normalized_snr =  \r\n",
        "     \r\n",
        "      yield (normalized_amp,normalized_snr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yKFTf7d1vMV"
      },
      "source": [
        "## Calling with Start as 1,2,3 will load in distinct sets of 10 (Or mentioned) files \r\n",
        "\r\n",
        "def LoadData2():\r\n",
        "  H_Re=[]\r\n",
        "  H_Im=[]\r\n",
        "  SNR=[]\r\n",
        "  #print(\"Loading Files\")\r\n",
        "  while True:\r\n",
        "    for i in range(64):\r\n",
        "      filename = CTW_unlabelled + \"file_{}.hdf5\".format(i+1)\r\n",
        "      H_Re, H_Im, SNR= get_data(filename)\r\n",
        "      i+=1\r\n",
        "       # YOUR_CODE\r\n",
        "      amplitude = \r\n",
        "      filtered_amp = \r\n",
        "      normalized_amp =\r\n",
        "\r\n",
        "      filtered_snr = \r\n",
        "      normalized_snr =  \r\n",
        "     \r\n",
        "      yield (normalized_snr,normalized_amp) #Note the difference in order of variables compared to loaddata1 function\r\n",
        "\r\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmGpnRZcqkIz"
      },
      "source": [
        "# Define and Train Pretext Task Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV2HOCcMqlsh"
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qKkfb-o2St5"
      },
      "source": [
        "## Model 1 (Uses H_Re and H_Im)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LENnyXyZMUTT"
      },
      "source": [
        "# Clear any logs from previous runs (Tensorboard)\r\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by5uMsSaidvG",
        "outputId": "5a7d27e4-0731-4b57-de8b-3255ec53c591"
      },
      "source": [
        "##Tried Leaky Relu -  Errors are too high , so used tanh instead\n",
        "\n",
        "#Base Model is the part we're using for transfer learning\n",
        "#i.e The part that will be transfered\n",
        "\n",
        "#YOUR_CODE\n",
        "#Add CNNs to the basemodel\n",
        "BaseModel1 = tf.keras.Sequential([tf.keras.layers.Flatten(),tf.keras.layers.Dense(924),tf.keras.layers.Activation('tanh'),tf.keras.layers.Dense(231)])\n",
        "\n",
        "\n",
        "model1 = tf.keras.Sequential([tf.keras.Input(shape=(56,924,5)),BaseModel1])\n",
        "model1.add(tf.keras.layers.Activation('tanh'))\n",
        "model1.add(tf.keras.layers.Dense(112))\n",
        "model1.add(tf.keras.layers.Activation('tanh'))\n",
        "model1.add(tf.keras.layers.Dense(56))\n",
        "\n",
        "print(model1.output_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQCVn_kWpwuX"
      },
      "source": [
        "print(BaseModel1.summary())\r\n",
        "print(model1.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o49yBIsf7Y6C"
      },
      "source": [
        "model1.compile(optimizer='sgd', loss='mse',metrics=['accuracy','mean_absolute_percentage_error'])\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCT-EPpn7aYU"
      },
      "source": [
        "## Train Pretext Model - Part 1 (Using Unlabelled data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozr7zd_OFx4x"
      },
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\r\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi4nqj61ukbU"
      },
      "source": [
        "savefilepath1 = \"/content/drive/MyDrive/Datasets/CTW2020/Checkpoints/PretextModel1/best_model.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(savefilepath1, monitor='loss', verbose=1,save_best_only=True, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7kEoDdBidvJ"
      },
      "source": [
        "gc.collect()       #Was running out of ram - Added to see if it helps\n",
        "model1.fit(LoadData1(),epochs=5,steps_per_epoch=64 ,verbose = 1,callbacks=[tensorboard_callback,checkpoint])\n",
        "model1.save_weights(savefilepath, overwrite=True, save_format=None, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp2tWzRDu25M"
      },
      "source": [
        "model1.load_weights(savefilepath1, by_name=False, skip_mismatch=False, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f72rJHzBw01"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0cu5yT7JI_L"
      },
      "source": [
        "## Model 2 (Uses SNR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0EjvU7mMMk7"
      },
      "source": [
        "# Clear any logs from previous run (Tensorboard) for the next train sequence (!!!CHECK FOR MORE EFFICIENT WAY)\r\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roThhIqbJF7N",
        "outputId": "8df09411-3111-4db2-b27d-43d151e8c424"
      },
      "source": [
        "#YOUR_CODE\r\n",
        "#Add CNNs to the basemodel\r\n",
        "BaseModel2 = tf.keras.Sequential([tf.keras.layers.Dense(112),tf.keras.layers.Activation('tanh'),tf.keras.layers.Dense(231)])\r\n",
        "\r\n",
        "model2 = tf.keras.Sequential([tf.keras.Input(shape=(56,5)),BaseModel2])\r\n",
        "model2.add(tf.keras.layers.Activation('tanh'))\r\n",
        "model2.add(tf.keras.layers.Dense(924))\r\n",
        "model2.add(tf.keras.layers.Activation('tanh'))\r\n",
        "model2.add(tf.keras.layers.Dense(103488))\r\n",
        "model2.add(tf.keras.layers.Reshape((2,56,924)))\r\n",
        "print(model2.output_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 2, 56, 924)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqt76UK6LKJQ"
      },
      "source": [
        "print(BaseModel2.summary())\r\n",
        "print(model2.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vdZvck0LNTv"
      },
      "source": [
        "model2.compile(optimizer='sgd', loss='mse',metrics=['accuracy','mean_absolute_percentage_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALgCN9_DMg9k"
      },
      "source": [
        "## Train Pretext Model - Part 2 (Using Unlabelled data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHdzyWTkLdcA"
      },
      "source": [
        "savefilepath2 = \"/content/drive/MyDrive/Datasets/CTW2020/Checkpoints/PretextModel2/best_model.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(savefilepath2, monitor='loss', verbose=1,save_best_only=True, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoYE5M8FLhTw"
      },
      "source": [
        "gc.collect()       #Was running out of ram - Added to see if it helps\r\n",
        "model2.fit(LoadData2(),epochs=5,steps_per_epoch=64 ,verbose = 1,callbacks=[tensorboard_callback,checkpoint])\r\n",
        "model2.save_weights(savefilepath, overwrite=True, save_format=None, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCX-rZhoLqJ6"
      },
      "source": [
        "model1.load_weights(savefilepath2, by_name=False, skip_mismatch=False, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2ZMoMqnLx44"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReG1LdP4qwxM"
      },
      "source": [
        "# Read Labelled data - For downstream Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AqvUB3OkMQp"
      },
      "source": [
        "# Clear any logs from previous run (Tensorboard) for the next train sequence (!!!CHECK FOR MORE EFFICIENT WAY)\r\n",
        "!rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSZpQNbSiduy"
      },
      "source": [
        "CTW_labelled = \"/content/drive/My Drive/Datasets/CTW2020/Unzipped/CTW2020_labelled_data/\"\r\n",
        "## path might vary based on how you saved data in your drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnmxFT1Gidu2"
      },
      "source": [
        "def get_data(data_file):\n",
        "    \n",
        "    f = h5py.File(data_file, 'r')\n",
        "    H_Re = f['H_Re'][:] #shape (sample size, 56, 924, 5)\n",
        "    H_Im = f['H_Im'][:] #shape (sample size, 56, 924, 5)\n",
        "    SNR = f['SNR'][:] #shape (sample size, 56, 5)\n",
        "    Pos = f['Pos'][:] #shape(sample size, 3)\n",
        "    f.close()\n",
        "            \n",
        "    return H_Re, H_Im, SNR, Pos        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nneQ5KE9YV5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ea117f-f53c-4f11-b2cc-29a101d869b2"
      },
      "source": [
        "#lists all the files\r\n",
        "\r\n",
        "for dirname, _, filenames in os.walk(CTW_labelled):\r\n",
        "    # for filename in filenames:\r\n",
        "    #     print(os.path.join(dirname, filename))\r\n",
        "    print(\"Total number of files = {}\".format(len(filenames)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of files = 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Hx3UxS1fwBF"
      },
      "source": [
        "## Calling with Start as 1,2,3 will load in distinct sets of 10 (Or mentioned) files \r\n",
        "\r\n",
        "def LoadData2():\r\n",
        "  H_Re=[]\r\n",
        "  H_Im=[]\r\n",
        "  SNR=[]\r\n",
        "  #print(\"Loading Files\")\r\n",
        "  for i in range(64):\r\n",
        "    filename = CTW_unlabelled + \"file_{}.hdf5\".format(i+1)\r\n",
        "    H_Re, H_Im, SNR= get_data(filename)\r\n",
        "    i+=1\r\n",
        "    \r\n",
        "    #YOUR_CODE\r\n",
        "\r\n",
        "    #Do the data preprocessing as before\r\n",
        "\r\n",
        "    yield (SNR , AMP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6nCmq9q5t_m"
      },
      "source": [
        "def LoadData_FinalModel():\r\n",
        "\r\n",
        "  H_Re=[]\r\n",
        "  H_Im=[]\r\n",
        "  SNR=[]\r\n",
        "  Pos=[]\r\n",
        "  \r\n",
        "  while True:\r\n",
        "    #print(\"Loading Files\")\r\n",
        "    for i in range(9):\r\n",
        "      filename = CTW_labelled + \"file_{}.hdf5\".format(i+1)\r\n",
        "      H_Re, H_Im, SNR ,Pos= get_data(filename)\r\n",
        "      i+=1\r\n",
        "  \r\n",
        "  #YOUR_CODE\r\n",
        "\r\n",
        "    #Do the data preprocessing as before\r\n",
        "    \r\n",
        "  \r\n",
        "      yield ((AMP,SNR),Pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap8a5T1GOa2x"
      },
      "source": [
        "# Downstream Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PXsuwRaOZo5"
      },
      "source": [
        "BaseModel1.trainable = False\r\n",
        "BaseModel2.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a_s8iVBOkOz"
      },
      "source": [
        "Input1 = tf.keras.Input(shape=(2,56,924))\r\n",
        "Input2 = tf.keras.Input(shape=(56))\r\n",
        "\r\n",
        "Output = tf.keras.layers.concatenate([BaseModel1(Input1), BaseModel2(Input2)], axis = 1)\r\n",
        "\r\n",
        "Output = tf.keras.layers.Activation('tanh')(Output)\r\n",
        "Output = tf.keras.layers.Dense(100)(Output)\r\n",
        "Output = tf.keras.layers.Activation('tanh')(Output)\r\n",
        "Output = tf.keras.layers.Dense(56)(Output)\r\n",
        "Output = tf.keras.layers.Activation('tanh')(Output)\r\n",
        "Output = tf.keras.layers.Dense(3)(Output)\r\n",
        "\r\n",
        "FinalModel = tf.keras.Model(inputs=[Input1,Input2], outputs=Output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYL_VlimWe1V"
      },
      "source": [
        "FinalModel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2icFdjG6Xr_I"
      },
      "source": [
        "FinalModel.compile(optimizer='sgd', loss='mse',metrics=['accuracy','mean_absolute_percentage_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqubRueb7UPE"
      },
      "source": [
        "savefilepath_FinalModel = \"/content/drive/My Drive/Datasets/CTW2020/Checkpoints/DownstreamModel/best_model.hdf5\"\r\n",
        "checkpoint = ModelCheckpoint(savefilepath_FinalModel, monitor='loss', verbose=1,save_best_only=True, mode='auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpFe1bag7Dhv"
      },
      "source": [
        "gc.collect()       #Was running out of ram - Added to see if it helps\r\n",
        "FinalModel.fit(LoadData_FinalModel(),steps_per_epoch=9 ,epochs=5 ,verbose = 1,callbacks=[tensorboard_callback,checkpoint])\r\n",
        "FinalModel.save_weights(savefilepath_FinalModel, overwrite=True, save_format=None, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg8MDk348NeX"
      },
      "source": [
        "FinalModel.load_weights(savefilepath, by_name=False, skip_mismatch=False, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd8X9pluZ385"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t2W8tJnkJGJ"
      },
      "source": [
        "##Code for Tuning Part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbCkUIQxkIG-"
      },
      "source": [
        "# Clear any logs from previous run (Tensorboard) for the next train sequence (!!!CHECK FOR MORE EFFICIENT WAY)\r\n",
        "# !rm -rf ./logs/\r\n",
        "\r\n",
        "#Make trainable\r\n",
        "BaseModel1.trainable = True\r\n",
        "BaseModel2.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ__FeJQkNkX"
      },
      "source": [
        "FinalModel.compile(optimizer='sgd', loss='mse',metrics=['accuracy','mean_absolute_percentage_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXA9dClqkSQ7"
      },
      "source": [
        "gc.collect()       #Was running out of ram - Added to see if it helps\r\n",
        "FinalModel.fit(LoadData_FinalModel(),epochs=5 , steps_per_epoch=9 ,verbose = 1,callbacks=[tensorboard_callback,checkpoint])\r\n",
        "FinalModel.save_weights(savefilepath_FinalModel, overwrite=True, save_format=None, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1iio8-dTmL_"
      },
      "source": [
        "FinalModel.load_weights(savefilepath_FinalModel, by_name=False, skip_mismatch=False, options=None)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}